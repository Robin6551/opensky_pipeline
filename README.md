# OpenSky Flight Data Pipeline âœˆï¸

An end-to-end **data engineering pipeline** that ingests live flight data from the **OpenSky Network API**, processes it using an **ETL workflow**, and stores it in **PostgreSQL**, orchestrated with **Apache Airflow** and deployed using **Docker**.

This project demonstrates real-world data engineering concepts including orchestration, ETL design, SQL schema creation, and containerized pipelines.

---

## ğŸš€ Project Overview

The pipeline performs the following steps:

1. **Extract** real-time flight data from the OpenSky API
2. **Transform** raw JSON data into structured, clean records
3. **Load** processed data into PostgreSQL tables
4. Orchestrate the workflow using **Airflow DAGs**

---

## ğŸ§± Architecture

OpenSky API
â†“
Extract (Python)
â†“
Transform (Python)
â†“
Load (PostgreSQL)
â†“
Airflow DAG (Scheduling & Orchestration)


---

## ğŸ›  Tech Stack

- **Python**
- **Apache Airflow**
- **PostgreSQL**
- **SQL**
- **Docker & Docker Compose**
- **Git & GitHub**

---

## ğŸ“ Project Structure

OPENSKY_PIPELINE/
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ opensky_pipeline_dags.py # Airflow DAG definition
â”‚ â”‚
â”‚ â””â”€â”€ src/
â”‚ â”œâ”€â”€ extract.py # Extract data from OpenSky API
â”‚ â”œâ”€â”€ transform.py # Data cleaning & transformation
â”‚ â”œâ”€â”€ load.py # Load data into PostgreSQL
â”‚ â””â”€â”€ main.py # ETL entry point
â”‚
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ create_tables.sql # Database schema creation
â”‚
â”œâ”€â”€ logs/ # Airflow logs
â”œâ”€â”€ scripts/ # Helper / setup scripts
â”‚
â”œâ”€â”€ docker-compose.yml # Airflow & Postgres services
â”œâ”€â”€ dockerfile # Custom Airflow image
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # Environment variables (not committed)
â””â”€â”€ README.md


---

## ğŸ“¥ Extract

- Fetches live flight state vectors from the OpenSky Network API
- Parses JSON response
- Handles missing or null values

**Sample fields extracted:**
- ICAO24
- Callsign
- Origin country
- Latitude / Longitude
- Altitude
- Velocity
- Timestamp

---

## ğŸ”„ Transform

- Cleans invalid or incomplete records
- Converts timestamps to readable formats
- Prepares structured data for database insertion

Transformation logic is implemented in `transform.py`.

---

## ğŸ“¤ Load

- Creates tables using SQL scripts
- Inserts transformed data into PostgreSQL
- Ensures data consistency and schema alignment

Database schema is defined in:
sql/create_tables.sql


---

## â± Orchestration with Airflow

- DAG defined in `opensky_pipeline_dags.py`
- Tasks:
  - Extract
  - Transform
  - Load
- Supports scheduled or manual execution

---

## â–¶ï¸ How to Run

### 1. Clone the repository

git clone https://github.com/Robin6551/opensky_pipeline.git
cd opensky_pipeline
2. Configure environment variables

Create a .env file:

POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=opensky

3. Start services
docker-compose up -d

4. Access Airflow UI
http://localhost:8080


Trigger the OpenSky DAG from the Airflow dashboard.

ğŸ“Š Use Cases

Real-time flight monitoring

Aviation analytics

Learning Airflow-based ETL pipelines

Data engineering portfolio project

ğŸ¯ Key Learnings

Designing ETL pipelines

Working with real-time APIs

Airflow DAG orchestration

PostgreSQL schema design

Dockerized data workflows

ğŸ”® Future Improvements

Incremental data loading

Data quality checks

Add monitoring & alerts

Cloud deployment (AWS / GCP)

Analytics dashboard integration

ğŸ‘¤ Author

Robin
Aspiring Data Engineer
GitHub: https://github.com/Robin6551



